{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fd0b6e4-8b53-49c9-b03a-cc71a053b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion() \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d09cf820-4422-4635-bff7-baa10406a41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/mnt/g/onlinelessons/deep learning/dataset/train_full_malware/trainLabels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32aca3eb-3a9f-4b08-9e55-355b4673b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return [\n",
    "      torch.stack([x[0] for x in batch]),\n",
    "      torch.tensor(np.array([x[1] for x in batch]))\n",
    "      #torch.stack([x[audio] for x in batch]),\n",
    "      #torch.tensor([x[labels] for x in batch])\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f2f2ea1-f25a-4377-ab78-047b4c7ed05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/mnt/g/onlinelessons/deep learning/dataset/train_full_malware/ByteToImage/256_x/processed_data_resampled5/\"\n",
    "\n",
    "class image_custom_dataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with labels.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.image_csv = pd.read_csv(csv_file)\n",
    "        self.transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image_name = self.image_csv.iloc[idx, 1]      \n",
    "        image = Image.open(image_name)\n",
    "\n",
    "        \n",
    "        label = self.image_csv.iloc[idx, 3:12]\n",
    "        label = np.array(label, dtype=np.float16)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if image.shape[0] == 1:\n",
    "            image = torch.cat((image, image, image), dim=0)\n",
    "            \n",
    "        image = transforms.Compose([transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])(image)\n",
    "                \n",
    "        # Add a batch dimension to the tensor\n",
    "        # image = image.unsqueeze(0)\n",
    "        \n",
    "        return_tuple = tuple([image , label])\n",
    "        \n",
    "\n",
    "        return return_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18bdfe4a-837a-4e7d-b9db-ad2069f75a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/mnt/g/onlinelessons/deep learning/dataset/train_full_malware/ByteToImage/256_x/processed_data_resampled5/train\"\n",
    "test_dir = \"/mnt/g/onlinelessons/deep learning/dataset/train_full_malware/ByteToImage/256_x/processed_data_resampled5/test\"\n",
    "val_dir = \"/mnt/g/onlinelessons/deep learning/dataset/train_full_malware/ByteToImage/256_x/processed_data_resampled5/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86395261-79e9-4826-8dba-8010889767ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets = {x.split(\"_\")[0]: image_custom_dataset(os.path.join(data_dir, x))\n",
    "                      for x in [\"train_data_encoded.csv\", \"test_data_encoded.csv\", \"val_data_encoded.csv\"]}\n",
    "batch_size = 16\n",
    "\n",
    "dataloaders = {\"test\": torch.utils.data.DataLoader(image_datasets[\"test\"], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=8, collate_fn=collate_fn)}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"test\"]}\n",
    "#class_names = audio_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(dataset_sizes[\"test\"])                  \n",
    "len(dataloaders[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "433bce63-0a49-4571-9d5f-13fac97e926b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50704931-4752-4f33-8fd3-4718131a972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_module(model_list, confidence_factor=0.8):\n",
    "    \n",
    "    was_training =[]\n",
    "    \n",
    "    for models in model_list:\n",
    "        was_training.append(models.training)\n",
    "        models.eval()\n",
    "    \n",
    "    model = model_list[0]\n",
    "    \n",
    "    true_labels = []\n",
    "    total_predict = []\n",
    "    pred_in_each_step = [0, 0, 0, 0]\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    fig = plt.figure()\n",
    "    model = model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        since = time.time()\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs\n",
    "\n",
    "            chosen_label = torch.argmax(labels, dim=1)\n",
    "            # print('\\033[30m' + f'label:     {chosen_label}' + '\\033[0m')       if you uncomment this line you can see original labels of each epoch\n",
    "            \n",
    "            _, predicted_labels = torch.max(preds, dim=1)\n",
    "            predicted_labels = predicted_labels.clone().detach()\n",
    "            # print(predicted_labels)\n",
    "            # print(chosen_label)\n",
    "            # print(inputs.shape)\n",
    "            for j , batch in enumerate(preds):\n",
    "                # print(predicted_labels[1][j])\n",
    "                if (float(batch[int(predicted_labels[j])]) < confidence_factor):\n",
    "                    pred_freq_dict = {}\n",
    "                    pred_freq_dict[int(predicted_labels[j])] = pred_freq_dict.get(int(predicted_labels[j]), 0) + 1\n",
    "                    stronger_model = model_list[1]\n",
    "                    stronger_model = stronger_model.to(device)\n",
    "                    strong_inputs = inputs[j]\n",
    "                    strong_inputs = strong_inputs.unsqueeze(0)\n",
    "                    # print(strong_inputs.shape)\n",
    "                    strong_outputs = stronger_model(strong_inputs)\n",
    "                    strong_preds = strong_outputs\n",
    "                    _, strong_labels = torch.tensor(torch.max(strong_preds, dim=1))\n",
    "#                     predicted_labels[j] = strong_labels\n",
    "#                     pred_in_each_step[1] += 1\n",
    "                    \n",
    "                    # print(strong_labels)\n",
    "        \n",
    "                    if (float(strong_preds[0][int(strong_labels)])< confidence_factor):\n",
    "                        pred_freq_dict[int(strong_labels)] = pred_freq_dict.get(int(strong_labels), 0) + 1\n",
    "                        stronger_model = model_list[2]\n",
    "                        stronger_model = stronger_model.to(device)\n",
    "                        # strong_inputs = inputs[j]\n",
    "                        # strong_inputs = strong_inputs.unsqueeze(0)\n",
    "                        # print(strong_inputs.shape)\n",
    "                        strong_outputs = stronger_model(strong_inputs)\n",
    "                        strong_preds = strong_outputs\n",
    "                        _, strong_labels = torch.tensor(torch.max(strong_preds, dim=1))\n",
    "                        predicted_labels[j] = strong_labels\n",
    "                        pred_in_each_step[2] += 1\n",
    "                        if (float(strong_preds[0][int(strong_labels)])< confidence_factor):\n",
    "                            pred_freq_dict[int(strong_labels)] = pred_freq_dict.get(int(strong_labels), 0) + 1\n",
    "                            stronger_model = model_list[3]\n",
    "                            stronger_model = stronger_model.to(device)\n",
    "                            # strong_inputs = inputs[j]\n",
    "                            # strong_inputs = strong_inputs.unsqueeze(0)\n",
    "                            # print(strong_inputs.shape)\n",
    "                            strong_outputs = stronger_model(strong_inputs)\n",
    "                            strong_preds = strong_outputs\n",
    "                            _, strong_labels = torch.tensor(torch.max(strong_preds, dim=1))\n",
    "                            \n",
    "                            if (float(strong_preds[0][int(strong_labels)])< confidence_factor):\n",
    "                                pred_freq_dict[int(strong_labels)] = pred_freq_dict.get(int(strong_labels), 0) + 1\n",
    "                                print(f\" predicted by voting\\ndict is: {pred_freq_dict}\")\n",
    "                                max_key = max(pred_freq_dict.values())\n",
    "                                max_keys = [key for key, value in pred_freq_dict.items() if value == max_key]\n",
    "                                if len(max_keys) == 1:\n",
    "                                    predicted_labels[j] = max_keys[0]\n",
    "                                else:\n",
    "                                    predicted_labels[j] = strong_labels\n",
    "                                pred_in_each_step[3] += 1                             \n",
    "                                \n",
    "                            else:\n",
    "                                pred_in_each_step[3] += 1\n",
    "                                predicted_labels[j] = strong_labels\n",
    "\n",
    "                        else:\n",
    "                            predicted_labels[j] = strong_labels\n",
    "                            pred_in_each_step[2] += 1\n",
    "\n",
    "                    else:\n",
    "                        predicted_labels[j] = strong_labels\n",
    "                        pred_in_each_step[1] += 1\n",
    "\n",
    "                else:\n",
    "                    pred_in_each_step[0] += 1\n",
    "                    \n",
    "            time_elapsed = time.time() - since   \n",
    "        \n",
    "            # print('\\033[31m' + f'predicted: {predicted_value}' + '\\033[0m')    if you uncomment this line you can see predicted label for each epoch\n",
    "\n",
    "            # print('\\033[33m' + f'predicted: {preds}' + '\\033[0m')              if you uncomment this line you can see predictions that are assigend to labels\n",
    "\n",
    "            running_corrects = torch.sum(chosen_label == predicted_labels)         \n",
    "            correct = correct + running_corrects\n",
    "            # print('\\033[32m' + f'corrects:: {correct}' + '\\033[0m')            if you uncomment this line you can see totall corrects on that epoch\n",
    "\n",
    "            total = total + inputs.size(0)\n",
    "            # print('\\033[34m' + f'total: {total}' + '\\033[0m')                  if you uncomment this line you can see total number of data processed in each epoch \n",
    "\n",
    "            true_labels.extend(chosen_label.to(\"cpu\").numpy())\n",
    "            total_predict.extend(predicted_labels.to(\"cpu\").numpy())\n",
    "\n",
    "        print('\\033[34m' + f'total: {total}' + '\\033[0m')\n",
    "\n",
    "        print('\\033[32m' + f'corrects:: {correct}' + '\\033[0m')  \n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print('\\033[35m' + f'acuuracy: {accuracy}' + '\\033[0m')\n",
    "        \n",
    "        for k, steps_power in enumerate(pred_in_each_step):\n",
    "            print('\\033[36m' + f\"{pred_in_each_step[k]} predicted in {k}th layer\" + '\\033[0m')\n",
    "            \n",
    "        print('\\033[34m' + f'total run-time = {time_elapsed}' + '\\033[0m')\n",
    "        print(f\"datas handled by each layer = {pred_in_each_step}\")\n",
    "        \n",
    "        for i, train_mode in enumerate(was_training):\n",
    "            model_list[i].train(mode=train_mode)\n",
    "\n",
    "    true_labels = [x+1 for x in true_labels]\n",
    "    total_predict = [x+1 for x in total_predict]\n",
    "\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    sns.set_style('dark') # darkgrid, white grid, dark, white and ticks\n",
    "    plt.rc('axes', titlesize=13)     # fontsize of the axes title\n",
    "    plt.rc('axes', labelsize=14)    # fontsize of the x and y labels\n",
    "    plt.rc('xtick', labelsize=11)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=11)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=13)    # legend fontsize\n",
    "    plt.rc('font', size=9)   \n",
    "\n",
    "    classes = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    ticks = [x-1 for x in classes]\n",
    "\n",
    "    conf_matrix = confusion_matrix(true_labels, total_predict, labels = classes)\n",
    "    df_cm = pd.DataFrame(conf_matrix, index = classes,\n",
    "                  columns = classes)\n",
    "    \n",
    "    num_classes = conf_matrix.shape[0]\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    f1_score = np.zeros(num_classes)\n",
    "    for i in range(num_classes):\n",
    "        tp = conf_matrix[i, i]\n",
    "        fp = np.sum(conf_matrix[:, i]) - tp\n",
    "        fn = np.sum(conf_matrix[i, :]) - tp\n",
    "        precision[i] = tp / (tp + fp)\n",
    "        recall[i] = tp / (tp + fn)\n",
    "        f1_score[i] = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i] + 1e-12)\n",
    "        \n",
    "    micro_precision = np.sum(conf_matrix.diagonal()) / np.sum(conf_matrix)\n",
    "    micro_recall = np.sum(conf_matrix.diagonal()) / np.sum(conf_matrix, axis=0).sum()\n",
    "    micro_f1_score = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-12)\n",
    "    \n",
    "    print('\\033[31m' + f'micro precision ={micro_precision}' + '\\033[0m')\n",
    "    print('\\033[32m' + f'micro recal ={micro_recall}' + '\\033[0m')\n",
    "    print('\\033[33m' + f'micro f1 score ={micro_f1_score}' + '\\033[0m')\n",
    "    \n",
    "    # Calculate macro-averaged precision, recall, and F1 score\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    macro_f1_score = 2 * (macro_precision * macro_recall) / (macro_precision + macro_recall + 1e-12)\n",
    "    \n",
    "    print('\\033[34m' + f'Macro precision ={macro_precision}' + '\\033[0m')\n",
    "    print('\\033[35m' + f'Macro recal ={macro_recall}' + '\\033[0m')\n",
    "    print('\\033[36m' + f'Macro f1 score ={macro_f1_score}' + '\\033[0m')\n",
    "\n",
    "    sns.heatmap(df_cm, annot=True, cmap='Blues', fmt='g') \n",
    "    \n",
    "    return {\"run-time\": time_elapsed,\n",
    "            \"macro_precision\": macro_precision,\n",
    "            \"macro_recall\": macro_recall,\n",
    "            \"macro_f1_score\": macro_f1_score,\n",
    "            \"micro_precision\": micro_precision,\n",
    "            \"micro_recall\": micro_recall,\n",
    "            \"micro_f1_score\": micro_f1_score,\n",
    "            \"accuracy\": float(accuracy),\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9144b25a-c0a1-4534-9d81-37f2a18162ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FireModule(nn.Module):\n",
    "    def __init__(self, in_channels, s1x1, e1x1, e3x3):\n",
    "        super(FireModule, self).__init__()\n",
    "        self.squeeze = nn.Conv2d(in_channels, s1x1, kernel_size=1)\n",
    "        self.expand_1x1 = nn.Conv2d(s1x1, e1x1, kernel_size=1)\n",
    "        self.expand_3x3 = nn.Conv2d(s1x1, e3x3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.squeeze(x))\n",
    "        return torch.cat([\n",
    "            F.relu(self.expand_1x1(x)),\n",
    "            F.relu(self.expand_3x3(x))\n",
    "        ], dim=1)\n",
    "\n",
    "\n",
    "class SqueezeNet(nn.Module):\n",
    "    def __init__(self, num_classes=9):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        self.num_classes = 9\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "            FireModule(96, 64, 128, 128),\n",
    "            # FireModule(64, 16, 64, 64),\n",
    "            # FireModule(128, 16, 64, 64),\n",
    "            # nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "            # FireModule(256, 32, 128, 128),\n",
    "            # FireModule(256, 48, 192, 192),\n",
    "            # FireModule(384, 48, 192, 192),\n",
    "            # FireModule(384, 64, 256, 256),\n",
    "        #     nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "        #     # FireModule(512, 64, 256, 256)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Conv2d(256,self.num_classes, kernel_size=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout(p=0.5),\n",
    "            # nn.Conv2d(128, self.num_classes, kernel_size=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            # torch.nn.Softmax(dim = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x.view(-1, self.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "455be7e1-1e00-4904-a2ff-6d51cf6fa7e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): FireModule(\n",
       "      (squeeze): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand_1x1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand_3x3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(256, 9, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "model1 = SqueezeNet()\n",
    "model1 = model1.to(device)\n",
    "\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a1c0ac6-bc64-4e73-9255-ebf1e6a82f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1000])\n",
      "5636720\n",
      "torch.Size([5, 1000])\n",
      "2382944\n",
      "torch.Size([5, 1000])\n",
      "1331472\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_nxn_bn(inp, oup, kernal_size=3, stride=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, kernal_size, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.SiLU()\n",
    "    )\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b p n (h d) -> b p h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b p h n d -> b p n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout))\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class MV2Block(nn.Module):\n",
    "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expansion)\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.SiLU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileViTBlock(nn.Module):\n",
    "    def __init__(self, dim, depth, channel, kernel_size, patch_size, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.ph, self.pw = patch_size\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(channel, channel, kernel_size)\n",
    "        self.conv2 = conv_1x1_bn(channel, dim)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, 4, 8, mlp_dim, dropout)\n",
    "\n",
    "        self.conv3 = conv_1x1_bn(dim, channel)\n",
    "        self.conv4 = conv_nxn_bn(2 * channel, channel, kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x.clone()\n",
    "\n",
    "        # Local representations\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # Global representations\n",
    "        _, _, h, w = x.shape\n",
    "        x = rearrange(x, 'b d (h ph) (w pw) -> b (ph pw) (h w) d', ph=self.ph, pw=self.pw)\n",
    "        x = self.transformer(x)\n",
    "        x = rearrange(x, 'b (ph pw) (h w) d -> b d (h ph) (w pw)', h=h//self.ph, w=w//self.pw, ph=self.ph, pw=self.pw)\n",
    "\n",
    "        # Fusion\n",
    "        x = self.conv3(x)\n",
    "        x = torch.cat((x, y), 1)\n",
    "        x = self.conv4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MobileViT(nn.Module):\n",
    "    def __init__(self, image_size, dims, channels, num_classes, expansion=4, kernel_size=3, patch_size=(2, 2)):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        ph, pw = patch_size\n",
    "        assert ih % ph == 0 and iw % pw == 0\n",
    "\n",
    "        L = [2, 4, 3]\n",
    "\n",
    "        self.conv1 = conv_nxn_bn(3, channels[0], stride=2)\n",
    "\n",
    "        self.mv2 = nn.ModuleList([])\n",
    "        self.mv2.append(MV2Block(channels[0], channels[1], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[1], channels[2], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))\n",
    "        self.mv2.append(MV2Block(channels[2], channels[3], 1, expansion))   # Repeat\n",
    "        self.mv2.append(MV2Block(channels[3], channels[4], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[5], channels[6], 2, expansion))\n",
    "        self.mv2.append(MV2Block(channels[7], channels[8], 2, expansion))\n",
    "        \n",
    "        self.mvit = nn.ModuleList([])\n",
    "        self.mvit.append(MobileViTBlock(dims[0], L[0], channels[5], kernel_size, patch_size, int(dims[0]*2)))\n",
    "        self.mvit.append(MobileViTBlock(dims[1], L[1], channels[7], kernel_size, patch_size, int(dims[1]*4)))\n",
    "        self.mvit.append(MobileViTBlock(dims[2], L[2], channels[9], kernel_size, patch_size, int(dims[2]*4)))\n",
    "\n",
    "        self.conv2 = conv_1x1_bn(channels[-2], channels[-1])\n",
    "\n",
    "        self.pool = nn.AvgPool2d(ih//32, 1)\n",
    "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.mv2[0](x)\n",
    "\n",
    "        x = self.mv2[1](x)\n",
    "        x = self.mv2[2](x)\n",
    "        x = self.mv2[3](x)      # Repeat\n",
    "\n",
    "        x = self.mv2[4](x)\n",
    "        x = self.mvit[0](x)\n",
    "\n",
    "        x = self.mv2[5](x)\n",
    "        x = self.mvit[1](x)\n",
    "\n",
    "        x = self.mv2[6](x)\n",
    "        x = self.mvit[2](x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = self.pool(x).view(-1, x.shape[1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def mobilevit_xxs():\n",
    "    dims = [64, 80, 96]\n",
    "    channels = [16, 16, 24, 24, 48, 48, 64, 64, 80, 80, 320]\n",
    "    return MobileViT((256, 256), dims, channels, num_classes=1000, expansion=2)\n",
    "\n",
    "\n",
    "def mobilevit_xs():\n",
    "    dims = [96, 120, 144]\n",
    "    channels = [16, 32, 48, 48, 64, 64, 80, 80, 96, 96, 384]\n",
    "    return MobileViT((256, 256), dims, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def mobilevit_s():\n",
    "    dims = [144, 192, 240]\n",
    "    channels = [16, 32, 64, 64, 96, 96, 128, 128, 160, 160, 640]\n",
    "    return MobileViT((256, 256), dims, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img = torch.randn(5, 3, 256, 256)\n",
    "    \n",
    "    vit = mobilevit_s()\n",
    "    out = vit(img)\n",
    "    print(out.shape)\n",
    "    print(count_parameters(vit))\n",
    "\n",
    "    vit = mobilevit_xs()\n",
    "    out = vit(img)\n",
    "    print(out.shape)\n",
    "    print(count_parameters(vit))\n",
    "    \n",
    "    vit = mobilevit_xxs()\n",
    "    out = vit(img)\n",
    "    print(out.shape)\n",
    "    print(count_parameters(vit))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6df14a18-a3e8-4b2f-bb3e-49f3ce207002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "Linear_layers_fc(\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Linear_layers_fc(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Linear_layers_fc, self).__init__()\n",
    "\n",
    "        # self.linear1 = torch.nn.Linear(1000, 256)\n",
    "        # self.bachNoem1 = nn.BatchNorm1d(256)\n",
    "        # self.activation1 = torch.nn.ReLU()\n",
    "        # self.dropout1 = nn.Dropout(p=0.2)\n",
    "\n",
    "        # self.linear5 = torch.nn.Linear(32, 9)\n",
    "\n",
    "        self.softmax = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x = self.linear5(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "linear_layers_ViT= Linear_layers_fc()\n",
    "\n",
    "print('The model:')\n",
    "print(linear_layers_ViT)\n",
    "\n",
    "# print('\\n\\nModel params:')\n",
    "# for param in MLP_model.parameters():\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "834f7c40-950e-4f15-84d6-6dcc5fa4e7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "CNN_last(\n",
      "  (conv): Conv2d(80, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU6()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN_last(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN_last, self).__init__()\n",
    "\n",
    "        # self.linear1 = torch.nn.Linear(1000, 256)\n",
    "        # self.bachNoem1 = nn.BatchNorm1d(256)\n",
    "        # self.activation1 = torch.nn.ReLU()\n",
    "        # self.dropout1 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.conv = nn.Conv2d(80, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        self.bn = nn.BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.activation = nn.ReLU6()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "CNN_last_layer_ViT= CNN_last()\n",
    "\n",
    "print('The model:')\n",
    "print(CNN_last_layer_ViT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b3f2f2a-29ad-44ab-837b-0c31ef67689f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileViT(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): SiLU()\n",
       "  )\n",
       "  (mv2): ModuleList(\n",
       "    (0): MV2Block(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SiLU()\n",
       "        (6): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): MV2Block(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SiLU()\n",
       "        (6): Conv2d(32, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2-3): 2 x MV2Block(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SiLU()\n",
       "        (6): Conv2d(48, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): MV2Block(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
       "        (4): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SiLU()\n",
       "        (6): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): MV2Block(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SiLU()\n",
       "        (6): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): MV2Block(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): SiLU()\n",
       "        (6): Conv2d(128, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mvit): ModuleList(\n",
       "    (0): MobileViTBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(48, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x ModuleList(\n",
       "            (0): PreNorm(\n",
       "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Attention(\n",
       "                (attend): Softmax(dim=-1)\n",
       "                (to_qkv): Linear(in_features=64, out_features=96, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=32, out_features=64, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): PreNorm(\n",
       "              (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "                  (1): SiLU()\n",
       "                  (2): Dropout(p=0.0, inplace=False)\n",
       "                  (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "                  (4): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(64, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "      (conv4): Sequential(\n",
       "        (0): Conv2d(96, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (1): MobileViTBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x ModuleList(\n",
       "            (0): PreNorm(\n",
       "              (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Attention(\n",
       "                (attend): Softmax(dim=-1)\n",
       "                (to_qkv): Linear(in_features=80, out_features=96, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=32, out_features=80, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): PreNorm(\n",
       "              (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=80, out_features=320, bias=True)\n",
       "                  (1): SiLU()\n",
       "                  (2): Dropout(p=0.0, inplace=False)\n",
       "                  (3): Linear(in_features=320, out_features=80, bias=True)\n",
       "                  (4): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "      (conv4): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (2): MobileViTBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(80, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "      (transformer): Transformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-2): 3 x ModuleList(\n",
       "            (0): PreNorm(\n",
       "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Attention(\n",
       "                (attend): Softmax(dim=-1)\n",
       "                (to_qkv): Linear(in_features=96, out_features=96, bias=False)\n",
       "                (to_out): Sequential(\n",
       "                  (0): Linear(in_features=32, out_features=96, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): PreNorm(\n",
       "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): FeedForward(\n",
       "                (net): Sequential(\n",
       "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "                  (1): SiLU()\n",
       "                  (2): Dropout(p=0.0, inplace=False)\n",
       "                  (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "                  (4): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(96, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "      (conv4): Sequential(\n",
       "        (0): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv2): CNN_last(\n",
       "    (conv): Conv2d(80, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU6()\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear_layers_fc(\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import timm\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "model2 = mobilevit_xxs()\n",
    "\n",
    "model2.conv2 = CNN_last_layer_ViT\n",
    "model2.fc = linear_layers_ViT\n",
    "model2.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "model2 = model2.to(device)\n",
    "\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c469132d-a029-4458-b867-f8c771da2ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "Linear_layers_fc(\n",
      "  (linear2): Linear(in_features=1280, out_features=9, bias=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Linear_layers_fc(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Linear_layers_fc, self).__init__()\n",
    "\n",
    "\n",
    "        # self.linear1 = torch.nn.Linear(1280, 128)\n",
    "        # self.bachNoem1 = nn.BatchNorm1d(128)\n",
    "        # self.activation1 = torch.nn.ReLU()\n",
    "        # self.dropout1 = nn.Dropout(p=0.2)\n",
    "\n",
    "        self.linear2 = torch.nn.Linear(1280, 9)\n",
    "        # self.bachNoem2 = nn.BatchNorm1d(64)\n",
    "        # self.activation2 = torch.nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "\n",
    "        # self.linear5 = torch.nn.Linear(256, 9)\n",
    "\n",
    "        self.softmax = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = self.dropout1(x)\n",
    "        # x = self.linear1(x)\n",
    "        # x = self.bachNoem1(x)\n",
    "        # x = self.activation1(x)\n",
    "\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear2(x)\n",
    "        # x = self.bachNoem2(x)\n",
    "        # x = self.activation2(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "linear_layers_mobilenet= Linear_layers_fc()\n",
    "\n",
    "print('The model:')\n",
    "print(linear_layers_mobilenet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "eae8e64c-7fc8-4f01-8e71-3a68f62c1f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ghostoftime111/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/ghostoftime111/anaconda3/envs/DL/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ghostoftime111/anaconda3/envs/DL/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model3 = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "\n",
    "num_classes = 9\n",
    "model3.classifier = linear_layers_mobilenet\n",
    "\n",
    "model3 = model3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5f72a1cf-f74b-449b-a597-461fac9d54be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghostoftime111/anaconda3/envs/DL/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "class_num = 9\n",
    "\n",
    "model4 = models.resnet18(pretrained=True)\n",
    "num_ftrs = model4.fc.in_features\n",
    "model4.fc = nn.Linear(num_ftrs, class_num)\n",
    "\n",
    "model4 = model4.to(device)\n",
    "model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4fd4aad5-722e-4302-8cb5-054b25823162",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_dir = \"./squeeznet_combined_best_256_better.pth\"\n",
    "model_2_dir = \"./vit-mobile-combined.pth\"\n",
    "model_3_dir = \"./mobilenet_v2_9_finetuned_combined_256.pth\"\n",
    "model_4_dir = \"./resnet18_finetuned-smote-256_new_better2.pth\"\n",
    "\n",
    "# model1 = torch.load(model_1_dir)\n",
    "# model2 = torch.load(model_2_dir)\n",
    "# model3 = torch.load(model_3_dir)\n",
    "# model4 = torch.load(model_4_dir)\n",
    "\n",
    "model1.load_state_dict(torch.load(model_1_dir))\n",
    "model1 = nn.Sequential(model1, nn.Softmax())\n",
    "model2.load_state_dict(torch.load(model_2_dir))\n",
    "model3.load_state_dict(torch.load(model_3_dir))\n",
    "model4.load_state_dict(torch.load(model_4_dir))\n",
    "model4 = nn.Sequential(model4, nn.Softmax())\n",
    "\n",
    "models_list = []\n",
    "models_list.append(model1)\n",
    "models_list.append(model2)\n",
    "models_list.append(model3)\n",
    "models_list.append(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cdabd13c-24b7-4335-b81e-18ea5132e585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghostoftime111/anaconda3/envs/DL/lib/python3.10/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " predicted by voting\n",
      "dict is: {8: 1, 5: 1, 0: 2}\n",
      " predicted by voting\n",
      "dict is: {2: 2, 8: 1, 6: 1}\n",
      " predicted by voting\n",
      "dict is: {3: 2, 1: 1, 7: 1}\n",
      " predicted by voting\n",
      "dict is: {8: 2, 0: 2}\n",
      " predicted by voting\n",
      "dict is: {5: 1, 3: 1, 7: 2}\n",
      " predicted by voting\n",
      "dict is: {4: 2, 3: 1, 0: 1}\n",
      " predicted by voting\n",
      "dict is: {1: 3, 0: 1}\n",
      " predicted by voting\n",
      "dict is: {7: 3, 1: 1}\n",
      " predicted by voting\n",
      "dict is: {7: 2, 0: 1, 1: 1}\n",
      " predicted by voting\n",
      "dict is: {5: 1, 7: 1, 0: 2}\n",
      " predicted by voting\n",
      "dict is: {5: 4}\n",
      "\u001b[34mtotal: 1094\u001b[0m\n",
      "\u001b[32mcorrects:: 1064\u001b[0m\n",
      "\u001b[35macuuracy: 97.25777435302734\u001b[0m\n",
      "\u001b[36m802 predicted in 0th layer\u001b[0m\n",
      "\u001b[36m253 predicted in 1th layer\u001b[0m\n",
      "\u001b[36m64 predicted in 2th layer\u001b[0m\n",
      "\u001b[36m14 predicted in 3th layer\u001b[0m\n",
      "\u001b[34mtotal run-time = 25.25873041152954\u001b[0m\n",
      "datas handled by each layer = [802, 253, 64, 14]\n",
      "\u001b[31mmicro precision =0.9725776965265083\u001b[0m\n",
      "\u001b[32mmicro recal =0.9725776965265083\u001b[0m\n",
      "\u001b[33mmicro f1 score =0.9725776965260082\u001b[0m\n",
      "\u001b[34mMacro precision =0.9524456931235963\u001b[0m\n",
      "\u001b[35mMacro recal =0.9720552987042225\u001b[0m\n",
      "\u001b[36mMacro f1 score =0.96215059037097\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = pipeline_module(models_list, confidence_factor=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756e40a3-4703-4dfe-9564-fe19f9d0474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 44\n",
    "conf_list = [0.4, 0.6, 0.8, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99]\n",
    "df = pd.read_excel(\"./pipeline - Copy.xlsx\")\n",
    "for i in range(13):\n",
    "    results = pipeline_module(models_list, confidence_factor=conf_list[i])\n",
    "    df.iloc[row + i] = results\n",
    "    \n",
    "df.to_excel(\"text.xlsx\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830a715-f56d-4366-a5f3-aa41d2d1f3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7775fce-055b-424b-9e9f-bd46cbcbc09b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
